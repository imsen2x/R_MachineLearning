---
title: "Random Forest Wine Quality"
author: "Jansen Lopez"
date: "8/28/2020"
output: rmarkdown::github_document
---


## Random Forest pre-processing experiment
### Goal: Compare if a standardizing the input features will result into better model performance 

# <br/>

### * The results might vary due to randomness


### Load the dataset and the required libraries
```{r}
library(tidymodels)
library(tidyverse)
library(skimr)
library(vip)

red_wine <- read.csv("winequality-red.csv")
View(red_wine)

skim(red_wine)
```

### Check the quality of red wines across the dataset
```{r}
red_wine %>%
  mutate(quality_description = case_when(
    quality < 5 ~ "Poor",
    quality >= 5 & quality < 8 ~ "Average",
    quality >= 8 ~ "Great"
  ),
  quality_description = factor(quality_description,
                                  levels = c("Poor", "Average", "Great"))) %>%
ggplot(aes(quality, fill = quality_description)) +
  geom_histogram() +
  scale_fill_manual(values = c("tomato", "cornflowerblue", "springgreen2"))
```

### Create a new dataset having the column red_wine_engineered
```{r}
red_wine_engineered <- red_wine %>%
  mutate(quality_description = case_when(
    quality < 5 ~ "Poor",
    quality >= 5 & quality < 8 ~ "Average",
    quality >= 8 ~ "Great"
  ),
  quality_description = factor(quality_description,
                               levels = c("Poor", "Average", "Great")))
```


### Average quality of red_wines are the most common 
```{r}
red_wine_engineered %>%
  count(quality_description) %>%
  mutate(pct = 100.0 * n / sum(n))
```
### Make a stratified training and testing set, then use recipes to standardize the features

```{r}
set.seed(1111)
# Stratify by quality description
data_split <- initial_split(red_wine_engineered, strata = quality_description)
red_wine_training <- training(data_split)
red_wine_testing <- testing(data_split)

# Prep the features by normalizing the dataset
red_wine_prepped <- recipe(quality ~ 
                             fixed.acidity + volatile.acidity + citric.acid +
                             residual.sugar + chlorides + free.sulfur.dioxide +
                             total.sulfur.dioxide + density + pH + sulphates +
                             alcohol,
                           data = red_wine_training) %>%
  step_normalize(all_numeric()) %>%
  step_zv(all_numeric()) %>%
  prep(retain = T)
```


### Use bootstrap samples for checking the overall model performance 
```{r}
red_wine_boot <- bootstraps(red_wine_training, times = 15, strata = quality_description) 
```


### Set the specifications for the Random Forest. Then explicitly state that min_n(minimum number of node for a split to continue) and mtry(number of random features used in building each tree) will be tuned. Then start training the models
```{r}
# Model specification
rf_spec <- rand_forest(
  min_n = tune(),
  trees = 1000,
  mtry = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Initial tuning
set.seed(3333)
tune_rf1 <- tune_grid(
  rf_spec,
  red_wine_prepped,
  resamples = red_wine_boot,
  grid = 15,
  metrics = metric_set(rmse, rsq),
  control = control_grid(save_pred = T, verbose = F)
)


```


### Quick sanity check. Check the variables that are the most important in our model
```{r}
# Sanity Check
best_model_exp <- finalize_model(
  rf_spec,
  select_best(tune_rf1, "rsq")
)

# Shows which variable are the most important in our model
best_model_exp %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(quality ~ .,
      data = juice(red_wine_prepped)) %>%
  vip(geom = "point")
```


### Create a function that shows the r-squared performance of min_n and mtry so we can get an idea what's the range where we can get the best r-squared performance for mtry and min_n
```{r}
rf_tuning_graph <- function(x){
  x %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
               values_to = "value",
               names_to = "parameter") %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = F) +
  facet_wrap(.~parameter, scales = "free_x")
}

```


### It seems that the r-squaredes are high for min_ns 11 - 15 and mtrys between 3 - 8. Let's disregard mtrys between 1 and 2 since using a small number of features in creating trees tend to overfit models
```{r}
# min_n: 11-15
# mtry: 6-8
rf_tuning_graph(tune_rf1)
```


### Check the best tuning parameters for r-squared and rmse
```{r}
tune_rf1 %>%
  select_best("rsq") 
tune_rf1 %>%
  select_best("rmse")

```


### Focus the retuning based on the initial results: min_n: 11-15 & mtry: 6-8
```{r}
rf_grid1 <- grid_regular(
  min_n(range = c(11, 15)),
  mtry(range = c(6, 8)),
  levels = 5
)

set.seed(1111)
tune_rf2 <- tune_grid(
  rf_spec,
  red_wine_prepped,
  resamples = red_wine_boot,
  grid = rf_grid1,
  metrics = metric_set(rmse, rsq),
  control = control_grid(save_pred = T, verbose = F)
)
```

### The best tuning configuration for the best r-squared are: min_n: 12 & mtry: 6. The best configuration for min_n and mtry for r-squared and rmse are the same. These really seems to be the best configuration for our model.

```{r}
collect_metrics(tune_rf2) 
select_best(tune_rf2, "rsq")
select_best(tune_rf2, "rmse")
```


### Check the most important features in our model
### alcohol and sulphates are the most important features
```{r}
best_model2 <- finalize_model(
  rf_spec,
  select_best(tune_rf2, "rsq")
)

best_model2 %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(quality ~ .,
      data = juice(red_wine_prepped)) %>%
  vip(geom = "point")

```

### Use the best parameters to model the test set
### Train with a Random Forest model using the standardized training set
### rsquared ~ 0.47
```{r}
rf_final_spec1 <- rand_forest(
  min_n = 12,
  trees = 1000,
  mtry = 6
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

set.seed(1111)
rf_final_model1 <- rf_final_spec1 %>%
  fit(quality ~ .,
       data = juice(red_wine_prepped))

rf_final_model1
```

### Get test set performance
```{r}
predicted_vals1 <- predict(rf_final_model1,
        new_data = bake(red_wine_prepped, new_data = red_wine_testing))

original_quality <- bake(red_wine_prepped, new_data = red_wine_testing) %>%
  select(quality)


predicted_vals1
rmse1 <- rmse(bind_cols(original_quality,
                        predicted_vals1), "quality", ".pred", na.rm = T)
rsq1 <- rsq(bind_cols(original_quality,
                      predicted_vals1), "quality", ".pred", na.rm = T)

juice(red_wine_prepped) %>% dim

n = 1200
p = 12
adjusted_rsq1 <- (1 - ((1 - rsq1$.estimate) * (n - 1) / (n - p - 1)))

```


### Test Set RMSE
```{r}
rmse1
```


### Test Set r-squared
```{r}
rsq1
```

### Test Set adjusted r-squared
```{r}
adjusted_rsq1
```


# Now we have used standardized features in our Random Forest model. Let's now see if there's a significant imporvement in the model's performance 

# <br>


### Again set the specifications for the Random Forest. Then explicitly state that min_n(minimum number of node for a split to continue) and mtry(number of random features used in building each tree) will be tuned. Then prep the dataset but this time we skip the standardization step
```{r}
rf_spec2 <- rand_forest(
  min_n = tune(),
  trees = 1000,
  mtry = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

red_wine_prepped2 <- recipe(quality ~ 
                              fixed.acidity + volatile.acidity + citric.acid +
                              residual.sugar + chlorides + free.sulfur.dioxide +
                              total.sulfur.dioxide + density + pH + sulphates +
                              alcohol,
                            data = red_wine_training) %>%
  prep(retain = T)

```


### Do the initial tuning of the non-standardized Random Forest Model
```{r}
set.seed(3333)
tune_rf3 <- tune_grid(
  rf_spec2,
  red_wine_prepped2,
  resamples = red_wine_boot,
  grid = 15,
  metrics = metric_set(rmse, rsq),
  control = control_grid(save_pred = T, verbose = F)
)

```

### Compare the initial tuning results of the the standardized and non-standardized model
### The r-squared values are roughly equal but the rmse is lower in the non-standardized dataset 
```{r}
collect_metrics(tune_rf1) %>%
  group_by(.metric) %>%
  summarize(average_metric = mean(mean)) 
collect_metrics(tune_rf3) %>%
  group_by(.metric) %>%
  summarize(average_metric = mean(mean))
```


### Another quick sanity check. Check the variables that are the most important in our model
```{r}
# Sanity Check
best_model_exp2 <- finalize_model(
  rf_spec2,
  select_best(tune_rf3, "rsq")
)

best_model_exp2 %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(quality ~ .,
      data = juice(red_wine_prepped2)) %>%
  vip(geom = "point")
```


### Compare the initial parameter configuations between the standardized and the non-standardized models
### They look roughly the same so we'll again choose min_n: 11-15 and mtry: 6-8 
```{r}
rf_tuning_graph(tune_rf1)
```


```{r}
rf_tuning_graph(tune_rf3)
```


### Check the best tuning parameters for r-squared and rmse
```{r}
tune_rf1 %>%
  select_best("rsq") 
tune_rf1 %>%
  select_best("rmse")

tune_rf3 %>%
  select_best("rsq") 
tune_rf3 %>%
  select_best("rmse")
```


### Focus the retuning based on the initial results: min_n: 11-15 & mtry: 6-8
```{r}
rf_grid2 <- grid_regular(
  min_n(range = c(11, 15)),
  mtry(range = c(6, 8)),
  levels = 15
)

set.seed(1111)
tune_rf4 <- tune_grid(
  rf_spec2,
  red_wine_prepped2,
  resamples = red_wine_boot,
  grid = rf_grid2,
  metrics = metric_set(rmse, rsq),
  control = control_grid(save_pred = T, verbose = F)
)
```


### The best tuning configuration for the best r-squared are: min_n: 12 & mtry: 6. Same  for the standardized model.
```{r}
collect_metrics(tune_rf4) 
select_best(tune_rf4, "rsq")
select_best(tune_rf4, "rmse")
```


### For the best configurations it seems that the r-squared is roughly the same for the standardized model and the non-standardized but the rmse of the non-standardized model is lower 
```{r}
collect_metrics(tune_rf2) %>%
  group_by(.metric) %>%
  summarize(average_metric = mean(mean))

collect_metrics(tune_rf4) %>%
  group_by(.metric) %>%
  summarize(average_metric = mean(mean))
```


### Another quick sanity check. Check the variables that are the most important in our model. Again alcohol and sulphates are still the most important features.
```{r}
best_model_non_stand <- finalize_model(
  rf_spec2,
  select_best(tune_rf4, "rsq")
)

best_model_non_stand %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(quality ~ .,
      data = juice(red_wine_prepped2)) %>%
  vip(geom = "point")

```


### Use the best parameters to model the test set
### Train with a Random Forest model using the non-standardized training set
### rsquared ~ 0.47
```{r}
rf_final_spec2 <- rand_forest(
  min_n = 12,
  trees = 1000,
  mtry = 6
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

set.seed(1111)
rf_final_model2 <- rf_final_spec2 %>%
  fit(quality ~ .,
      data = juice(red_wine_prepped2))


rf_final_model2
```

### Re-check the final model of the standardized dataset
```{r}
rf_final_model1
```


### Get test set performance of the non-standardized dataset
```{r}
predicted_vals2 <- predict(rf_final_model2,
                           new_data = bake(red_wine_prepped2, new_data = red_wine_testing))

original_quality2 <- bake(red_wine_prepped2, new_data = red_wine_testing) %>%
  select(quality)


predicted_vals2
rmse2 <- rmse(bind_cols(original_quality2,
                        predicted_vals2), "quality", ".pred", na.rm = T)
rsq2 <- rsq(bind_cols(original_quality2,
                      predicted_vals2), "quality", ".pred", na.rm = T)

juice(red_wine_prepped2) %>% dim

n = 1200
p = 12
adjusted_rsq2 <- (1 - ((1 - rsq2$.estimate) * (n - 1) / (n - p - 1)))
```



### Test Set RMSE
```{r}
rmse2
```



### Test Set r-squared
```{r}
rsq2
```



### Test Set adjusted r-squared
```{r}
adjusted_rsq2
```
# Takeaways

### It seems that standardizing a dataset before fitting it into a Tree-Based Model doesn't improve the model.In fact based on this test it the metrics were slightly worse-since we got a higher RMSE on the standardized model and the predictions lost its raw value. This seems to prove that Tree-Based models are shielded from the magnitude effects from our features and better feature selection might be the key to get better models.










